{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d5a0f29",
   "metadata": {},
   "source": [
    "## 1. Cite 5 diferenças entre GBM e Adaboost:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e312925",
   "metadata": {},
   "source": [
    "    - 1.O Adaboost é uma floresta de stumps, enquanto o GBM é uma floresta de árvores;\n",
    "    \n",
    "    - 2.Enquanto o 1º passo do Adaboost cria um stump, o 1º passo do GBM gera a média das variáveis resposta;\n",
    "    \n",
    "    - 3.No Adaboost cada stump tem um peso diferente, enquanto no GBM todas as respostas possuem o mesmo peso, mas um multiplicador em comum (learning rate/eta);\n",
    "    \n",
    "    - 4.O GBM é mais propenso ao overfitting, em relação ao Adaboost que é mais robusto;\n",
    "    \n",
    "    - 5.GBM costuma ser mais lento de treinar do que AdaBoost, devido ao GBM precisar calcular o gradiente do erro do modelo para cada árvore de decisão, em contrapartida o Adaboost precisa apenas calcular o erro do modelo em cada stump. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c849aa8",
   "metadata": {},
   "source": [
    "## 2. Acesse o link Scikit-learn – GBM, leia a explicação(traduza se for preciso) e crie um jupyter notebook contendo o exemplo de classificação e de regressão do GBM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22b1e55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78107c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.009154859960321"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_friedman1\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X, y = make_friedman1(n_samples=1200, random_state=0, noise=1.0)\n",
    "\n",
    "X_train, X_test = X[:200], X[200:]\n",
    "y_train, y_test = y[:200], y[200:]\n",
    "\n",
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='squared_error').fit(X_train, y_train)\n",
    "mean_squared_error(y_test, est.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac6a2bd",
   "metadata": {},
   "source": [
    "## 3. Cite 5 Hyperparametros importantes no GBM:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55863411",
   "metadata": {},
   "source": [
    "    - 1. loss: função de perda; está relacionada ao desvio binomial e multinomial, utilizada também na regressão logística. Tem boa utilização em resultados probabilísticos.\n",
    "    \n",
    "    - 2. subsample: a fração de amostras a ser usada para regular os learners básicos individuais. Se < 1, resultará em Stochastic Gradient Boosting, o que resulta na redução da variância e aumento do viés. Seus valores devem estar entre 0 e 1.\n",
    "    \n",
    "    - 3. criterion:sua utilidade é mensurar a qualidade da quebra. Os critérios suportados são 'friedman_mse' para o erro quadrático médio com pontuação de melhoria de Friedman e 'squared_error' para o mse.  \n",
    "    \n",
    "    - 4. min_weight_fraction_leaf: fração ponderada mínima da soma total dos pesos (de todas as amostras de entrada) necessária para estar em uma folha. As amostras tem peso igual quando sample_weight = 0 (padrão). Os valores devem estar no intervalo [0.0, 0.5].\n",
    "    \n",
    "    - 5. min_impurity_decrease: haverá uma quebra do nó se o valor de impureza for maior ou igual ao valor definido neste hiperparâmetro. Os valores devem estar no intervalo [0.0, inf]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f5d278",
   "metadata": {},
   "source": [
    "## 4. (Opcional) Utilize o GridSearch para encontrar os melhores hyperparametros para o conjunto de dados do exemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b154492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f46cf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alext\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "500 fits failed out of a total of 2500.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "500 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\alext\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\alext\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\", line 420, in fit\n",
      "    self._validate_params()\n",
      "  File \"C:\\Users\\alext\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 581, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\alext\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 97, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'n_estimators' parameter of GradientBoostingClassifier must be an int in the range [1, inf). Got 0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\alext\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.50566667 0.50566667 0.50566667 0.50566667        nan\n",
      " 0.50566667 0.50566667 0.50566667 0.50566667        nan 0.50566667\n",
      " 0.50566667 0.50566667 0.50566667        nan 0.50566667 0.50566667\n",
      " 0.50566667 0.50566667        nan 0.50566667 0.50566667 0.50566667\n",
      " 0.50566667        nan 0.50566667 0.50566667 0.50566667 0.50566667\n",
      "        nan 0.50566667 0.50566667 0.50566667 0.50566667        nan\n",
      " 0.50566667 0.50566667 0.50566667 0.50566667        nan 0.50566667\n",
      " 0.50566667 0.50566667 0.50566667        nan 0.50566667 0.50566667\n",
      " 0.50566667 0.50566667        nan 0.71558333 0.81658333 0.83216667\n",
      " 0.85825           nan 0.71558333 0.81658333 0.83216667 0.85825\n",
      "        nan 0.71558333 0.81658333 0.83216667 0.85825           nan\n",
      " 0.71558333 0.81658333 0.83216667 0.85825           nan 0.71558333\n",
      " 0.81658333 0.83216667 0.85825           nan 0.71558333 0.81658333\n",
      " 0.83216667 0.85825           nan 0.71558333 0.81658333 0.83216667\n",
      " 0.85825           nan 0.71558333 0.81658333 0.83216667 0.85825\n",
      "        nan 0.71558333 0.81658333 0.83216667 0.85825           nan\n",
      " 0.71558333 0.81658333 0.83216667 0.85825           nan 0.70575\n",
      " 0.70158333 0.69425    0.69425           nan 0.70575    0.70158333\n",
      " 0.69425    0.69425           nan 0.70575    0.70158333 0.69425\n",
      " 0.69425           nan 0.70575    0.70158333 0.69425    0.69425\n",
      "        nan 0.70575    0.70158333 0.69425    0.69425           nan\n",
      " 0.70575    0.70158333 0.69425    0.69425           nan 0.70575\n",
      " 0.70158333 0.69425    0.69425           nan 0.70575    0.70158333\n",
      " 0.69425    0.69425           nan 0.70575    0.70158333 0.69425\n",
      " 0.69425           nan 0.70575    0.70158333 0.69425    0.69425\n",
      "        nan 0.4785     0.4785     0.4785     0.4785            nan\n",
      " 0.4785     0.4785     0.4785     0.4785            nan 0.4785\n",
      " 0.4785     0.4785     0.4785            nan 0.4785     0.4785\n",
      " 0.4785     0.4785            nan 0.4785     0.4785     0.4785\n",
      " 0.4785            nan 0.4785     0.4785     0.4785     0.4785\n",
      "        nan 0.4785     0.4785     0.4785     0.4785            nan\n",
      " 0.4785     0.4785     0.4785     0.4785            nan 0.4785\n",
      " 0.4785     0.4785     0.4785            nan 0.4785     0.4785\n",
      " 0.4785     0.4785            nan 0.54058333 0.54058333 0.54058333\n",
      " 0.54058333        nan 0.54058333 0.54058333 0.54058333 0.54058333\n",
      "        nan 0.54058333 0.54058333 0.54058333 0.54058333        nan\n",
      " 0.54058333 0.54058333 0.54058333 0.54058333        nan 0.54058333\n",
      " 0.54058333 0.54058333 0.54058333        nan 0.54058333 0.54058333\n",
      " 0.54058333 0.54058333        nan 0.54058333 0.54058333 0.54058333\n",
      " 0.54058333        nan 0.54058333 0.54058333 0.54058333 0.54058333\n",
      "        nan 0.54058333 0.54058333 0.54058333 0.54058333        nan\n",
      " 0.54058333 0.54058333 0.54058333 0.54058333        nan 0.53175\n",
      " 0.53175    0.53175    0.53175           nan 0.53175    0.53175\n",
      " 0.53175    0.53175           nan 0.53175    0.53175    0.53175\n",
      " 0.53175           nan 0.53175    0.53175    0.53175    0.53175\n",
      "        nan 0.53175    0.53175    0.53175    0.53175           nan\n",
      " 0.53175    0.53175    0.53175    0.53175           nan 0.53175\n",
      " 0.53175    0.53175    0.53175           nan 0.53175    0.53175\n",
      " 0.53175    0.53175           nan 0.53175    0.53175    0.53175\n",
      " 0.53175           nan 0.53175    0.53175    0.53175    0.53175\n",
      "        nan 0.51558333 0.51558333 0.51558333 0.51558333        nan\n",
      " 0.51558333 0.51558333 0.51558333 0.51558333        nan 0.51558333\n",
      " 0.51558333 0.51558333 0.51558333        nan 0.51558333 0.51558333\n",
      " 0.51558333 0.51558333        nan 0.51558333 0.51558333 0.51558333\n",
      " 0.51558333        nan 0.51558333 0.51558333 0.51558333 0.51558333\n",
      "        nan 0.51558333 0.51558333 0.51558333 0.51558333        nan\n",
      " 0.51558333 0.51558333 0.51558333 0.51558333        nan 0.51558333\n",
      " 0.51558333 0.51558333 0.51558333        nan 0.51558333 0.51558333\n",
      " 0.51558333 0.51558333        nan 0.48825    0.48825    0.48825\n",
      " 0.48825           nan 0.48825    0.48825    0.48825    0.48825\n",
      "        nan 0.48825    0.48825    0.48825    0.48825           nan\n",
      " 0.48825    0.48825    0.48825    0.48825           nan 0.48825\n",
      " 0.48825    0.48825    0.48825           nan 0.48825    0.48825\n",
      " 0.48825    0.48825           nan 0.48825    0.48825    0.48825\n",
      " 0.48825           nan 0.48825    0.48825    0.48825    0.48825\n",
      "        nan 0.48825    0.48825    0.48825    0.48825           nan\n",
      " 0.48825    0.48825    0.48825    0.48825           nan 0.49666667\n",
      " 0.49666667 0.49666667 0.49666667        nan 0.49666667 0.49666667\n",
      " 0.49666667 0.49666667        nan 0.49666667 0.49666667 0.49666667\n",
      " 0.49666667        nan 0.49666667 0.49666667 0.49666667 0.49666667\n",
      "        nan 0.49666667 0.49666667 0.49666667 0.49666667        nan\n",
      " 0.49666667 0.49666667 0.49666667 0.49666667        nan 0.49666667\n",
      " 0.49666667 0.49666667 0.49666667        nan 0.49666667 0.49666667\n",
      " 0.49666667 0.49666667        nan 0.49666667 0.49666667 0.49666667\n",
      " 0.49666667        nan 0.49666667 0.49666667 0.49666667 0.49666667\n",
      "        nan 0.49091667 0.49091667 0.49091667 0.49091667        nan\n",
      " 0.49091667 0.49091667 0.49091667 0.49091667        nan 0.49091667\n",
      " 0.49091667 0.49091667 0.49091667        nan 0.49091667 0.49091667\n",
      " 0.49091667 0.49091667        nan 0.49091667 0.49091667 0.49091667\n",
      " 0.49091667        nan 0.49091667 0.49091667 0.49091667 0.49091667\n",
      "        nan 0.49091667 0.49091667 0.49091667 0.49091667        nan\n",
      " 0.49091667 0.49091667 0.49091667 0.49091667        nan 0.49091667\n",
      " 0.49091667 0.49091667 0.49091667        nan 0.49091667 0.49091667\n",
      " 0.49091667 0.49091667]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10min 52s\n",
      "Wall time: 10min 57s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=GradientBoostingClassifier(learning_rate=1.0,\n",
       "                                                  max_depth=1, random_state=0),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         &#x27;min_impurity_decrease&#x27;: [0, 1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                   9],\n",
       "                         &#x27;n_estimators&#x27;: [0, 10, 20, 30, 40]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=GradientBoostingClassifier(learning_rate=1.0,\n",
       "                                                  max_depth=1, random_state=0),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         &#x27;min_impurity_decrease&#x27;: [0, 1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                   9],\n",
       "                         &#x27;n_estimators&#x27;: [0, 10, 20, 30, 40]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=0)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=1.0, max_depth=1, random_state=0)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=GradientBoostingClassifier(learning_rate=1.0,\n",
       "                                                  max_depth=1, random_state=0),\n",
       "             param_grid={'learning_rate': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "                         'min_impurity_decrease': [0, 1, 2, 3, 4, 5, 6, 7, 8,\n",
       "                                                   9],\n",
       "                         'n_estimators': [0, 10, 20, 30, 40]},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "parameters = {\n",
    "    'n_estimators': list(range(0, 50, 10)),\n",
    "    'learning_rate': list(range(0, 10, 1)), \n",
    "    'min_impurity_decrease': list(range(0, 10, 1))\n",
    "    \n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator = clf,\n",
    "                    param_grid = parameters,\n",
    "                    scoring = 'accuracy',\n",
    "                    cv = 5)\n",
    "\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b11a701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 1, 'min_impurity_decrease': 0, 'n_estimators': 40}\n"
     ]
    }
   ],
   "source": [
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f201b3",
   "metadata": {},
   "source": [
    "##  5. Acessando o artigo do Jerome Friedman (Stochastic) e pensando no nome dado ao Stochastic GBM, qual é a maior diferença entre os dois algoritmos?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d8cef",
   "metadata": {},
   "source": [
    "O Stochastic GBM é a aplicação de 'aleatoriedade' no GBM convencional, aplicando um 'bagging' na criação das árvores de decisão e aumentando sua performance. o Stochastic GBM é bastante utilizado em modelos de previsão relacionados a bolsa de valores e previsões de vendas devido a este fator de aleatoriedade que não pode ser desconsiderado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84552aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
